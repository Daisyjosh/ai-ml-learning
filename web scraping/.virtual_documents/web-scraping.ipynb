import requests
from bs4 import BeautifulSoup
import pandas as pd
import os


# fetch data and store it
page_count = 1

while True:
    URL = f"https://quotes.toscrape.com/page/{page_count}/"
    res = requests.get(URL)

    soup = BeautifulSoup(res.text,"lxml")
    quote = soup.select("div.quote")

    if not quote:
        print("no valid pages anymore...")
        break
    with open(f"scraped-data/quotes{page_count}.html","w",encoding="utf-8") as f:
        f.write(res.text)
        print(f"downloaded data from page {page_count}")

    page_count = page_count + 1
    


import os

# extract information
page_count = 1
all_life_quotes = []
while True:
    
    file_path = f"scraped-data/quotes{page_count}.html"
    if not os.path.exists(file_path):
        print("no more pages to parse.")
        break
    with open(f"scraped-data/quotes{page_count}.html","r",encoding="utf-8") as f:
        html_content = f.read()
    soup = BeautifulSoup(html_content,"lxml")
    
    all_quotes = soup.select("div.quote")
    life_quotes = []
    for q in all_quotes:
        tags = [tag.get_text(strip=True) for tag in q.select(".tags .tag")]
        
        if "life" in all_tags:
            text = q.select_one("span.text").get_text(strip=True)
            author = q.select_one("small.author").get_text(strip=True)
            all_life_quotes.append(
                {
                    "quote" : text,
                    "author" : 
                }
            )
            
    
    page_count += 1

